<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<title><![CDATA[Tag: MPI | 记往开来]]></title>
<link href="http://xiaoliuai.github.io/tags/mpi/atom.xml" rel="self"/>
<link href="http://xiaoliuai.github.io/"/>
<updated>2016-01-20T18:36:47+01:00</updated>
<id>http://xiaoliuai.github.io/</id>
<author>
<name><![CDATA[Xiao Liu]]></name>

</author>
<generator uri="http://octopress.org/">Octopress</generator>

<entry>
<title type="html"><![CDATA[Parallelism]]></title>
<link href="http://xiaoliuai.github.io/blog/2014-11-26-parallelism.html"/>
<updated>2014-11-26T00:00:00+01:00</updated>
<id>http://xiaoliuai.github.io/blog/parallelism</id>
<content type="html"><![CDATA[<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Installation</a>
<ul>
<li><a href="#sec-1-1">1.1. Local Machine</a></li>
<li><a href="#sec-1-2">1.2. Virtual Machine</a></li>
</ul>
</li>
<li><a href="#sec-2">2. Ocatve</a>
<ul>
<li><a href="#sec-2-1">2.1. Run octave code with mpi.</a></li>
<li><a href="#sec-2-2">2.2. Programming</a></li>
<li><a href="#sec-2-3">2.3. Summary</a></li>
</ul>
</li>
<li><a href="#sec-3">3. C</a></li>
<li><a href="#sec-4">4. Performance Comparison</a></li>
</ul>
</div>
</div>


<p>
In this article, I will introduce the installation and basic use of MPI
framework through octave. Matlab is widely used in many scientific
domains as it enables the developers focus on the algorithms other
than programming skills. However, the support of parallelism computing
from Matlab is limited and expensive, especially for computing on
clusters. Currently, two parallelism frameworks are widely used: MPI
(Message Passage Interface) and MapReduce. Hadoop, which is an Java
implementation of MapReduce, does not currently support Matlab or
Octave. Therefore, I choose the MPI + Octave as a parallelism solution
for Matlab users.
</p>




<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><a href="xiaoliuai.github.io/tags/mpi/atom.xml"><span class="section-number-2">1</span> Installation</a></h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Local Machine</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Install open-mpi.
<ul class="org-ul">
<li>On Mac OS X with homebrew, <code>brew install open-mpi</code>
</li>
<li>On Ubuntu, the direct way is install the packages via package
manager <code>sudo apt-get install openmpi-bin openmpi-common
        openmpi-checkpoint libopenmpi-dev libopenmpi-X.X</code>. However,
the OpenMPI installed through this method does not work with
the octave-mpi package due to a library linkage problem. A
fast cue is compiling and installing it manually with specific
configurations.
Download the openmpi package <a href="http://www.open-mpi.org/software/ompi/v1.8/">openmpi-1.8.3.tar.gz</a>.
<div class="org-src-container">

<pre class="src src-bash">tar xzvf openmpi-1.8.3.tar.gz
cd openmpi-1.8.3
./configure --dsiable-dlopen
make
sudo make install
</pre>
</div>
</li>
</ul>
</li>
<li>Install octave
<ul class="org-ul">
<li>On Mac OS X with homebrew
<ol class="org-ol">
<li>install XQuartz from image file downloaded from <a href="http://xquartz.macosforge.org/landing/">http://xquartz.macosforge.org/landing/</a>.
</li>
<li><code>brew tap homebrew/science</code>
</li>
<li>install gfortran <code>brew install gcc</code>
</li>
<li><code>brew install octave</code>
</li>
</ol>
</li>
<li>On Ubuntu
<code>sudo apt-get install octave</code>
</li>
</ul>
</li>
<li>Install octave mpi package
<ul class="org-ul">
<li>Inside octave
<div class="org-src-container">

<pre class="src src-octave">putenv (<span style="color: #2aa198;">"PATH"</span><span style="color: #6c71c4;">,</span> [<span style="color: #2aa198;">"path/to/mpic++/:"</span> getenv(<span style="color: #2aa198;">"PATH"</span>)]) <span style="color: #93a1a1; font-style: italic;">% optional</span>&#57344;&#57345;&#57345;
pkg install <span style="color: #6c71c4;">-</span>forge mpi      <span style="color: #93a1a1; font-style: italic;">% install mpi from internet or</span>&#57344;&#57345;&#57345;
pkg install PATH<span style="color: #6c71c4;">/</span>TO<span style="color: #6c71c4;">/</span>PACKAGE <span style="color: #93a1a1; font-style: italic;">% install mpi from downloaded package</span>&#57344;&#57345;&#57345;
</pre>
</div>
</li>
<li>Through package manager in Ubuntu (deprecated), run <code>apt-get
        install octave-openmpi-ext</code>. As the pakcage manager could
automatically install the OpenMPI package, which does not work
with the octave-mpi actually, we suggest do NOT use this
method.
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Virtual Machine</h3>
</div>
</div>




<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Ocatve</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Run octave code with mpi.</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>First way is run code from shell/terminal/ssh.
<div class="org-src-container">

<pre class="src src-bash">mpirun [OPTIONS] octave -q --eval '[octave code, function or script name]'
</pre>
</div>
</li>
<li>Second way is call the system command inside octave.
<div class="org-src-container">

<pre class="src src-octave">system (<span style="color: #2aa198;">"mpirun octave -q --eval '[octave code, function or script name]"</span>)<span style="color: #6c71c4;">;</span>&#57344;&#57345;
</pre>
</div>
</li>
<li>Octave mpi package contains some simple examples to show you
how to write the octave code that enables the parallelism via
MPI. These codes are in two possible locations if you install
the octave mpi package globally:
<ul class="org-ul">
<li><code>/usr/share/doc/octave-mpi/examples/</code>
</li>
<li><code>/usr/share/octave/packages/mpi-1.2.0/</code>
</li>
</ul>
<p>
If you install the package under user&#8217;s home folder, you can
find exmaple codes under <code>$HOME/octave/packages/mpi-1.2.0/</code>. To
test the example inside octave, run command
<code>[info res] = system ("mpirun octave -q --eval 'pkg load mpi; helloworld
       ()'");</code>. <b>The displayed string of the funciton, e.g. <code>helloworld()</code>, will return by this invokation. Therefore your can embed it in another
serial computing octave script context (use str2num()).</b>
</p>
</li>
<li>Frequently used mpirun options: <code>--hostfile</code> indicate the file
that contains the hostnames of cluster; <code>-np</code> number of
processes to run.
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Programming</h3>
<div class="outline-text-3" id="text-2-2">
<p>
The interface provided by octave mpi package is a subset of all
MPI functions.
</p>

<p>
Example 1, basic example about initializing the environment, send
and receive messages/variables. Basically, these functions are
enough to create the parallelism program.
</p>
<div class="org-src-container">

<pre class="src src-octave">MPI_Init()<span style="color: #6c71c4;">;</span>&#57344;&#57345;&#57345;
communicator <span style="color: #6c71c4;">=</span> MPI_Comm_Load(<span style="color: #2aa198;">"a label string"</span>) <span style="color: #93a1a1; font-style: italic;">% one can use multiple communicators indicated by different label</span>&#57344;&#57345;
my_rank <span style="color: #6c71c4;">=</span> MPI_Comm_rank(communicator) <span style="color: #93a1a1; font-style: italic;">% rank is the indicator(id) of current process</span>&#57344;&#57345;
p <span style="color: #6c71c4;">=</span> MPI_Comm_size(CW) <span style="color: #93a1a1; font-style: italic;">% the size of a communicator is the number of processes handled by the communicator</span>&#57344;&#57345;
[info] <span style="color: #6c71c4;">=</span> MPI_Send(value<span style="color: #6c71c4;">,</span> ranks<span style="color: #6c71c4;">,</span> TAG<span style="color: #6c71c4;">,</span> communicator)<span style="color: #6c71c4;">;</span>&#57344;&#57345;&#57345;
<span style="color: #93a1a1; font-style: italic;">% </span><span style="color: #93a1a1; font-style: italic;">value:  a octave variable</span>&#57344;&#57345;&#57345;
<span style="color: #93a1a1; font-style: italic;">% </span><span style="color: #93a1a1; font-style: italic;">ranks:  a vector that contains the id of destination processes, can be an integer</span>&#57344;&#57345;
<span style="color: #93a1a1; font-style: italic;">% </span><span style="color: #93a1a1; font-style: italic;">TAG:    an integer to identify the message (when there are many messages sent in the same context)</span>&#57344;&#57345;
<span style="color: #93a1a1; font-style: italic;">% </span><span style="color: #93a1a1; font-style: italic;">info:   an integer that indicates the success of failure state of process</span>&#57344;&#57345;
[value<span style="color: #6c71c4;">,</span> info] <span style="color: #6c71c4;">=</span> MPI_Recv(source<span style="color: #6c71c4;">,</span> TAG<span style="color: #6c71c4;">,</span> communicator)&#57344;&#57345;&#57345;
<span style="color: #93a1a1; font-style: italic;">% </span><span style="color: #93a1a1; font-style: italic;">source: the id the process that send this message</span>&#57344;&#57345;&#57345;
<span style="color: #93a1a1; font-style: italic;">%         </span><span style="color: #93a1a1; font-style: italic;">receive from any process if source is -1</span>&#57344;&#57345;&#57345;
<span style="color: #93a1a1; font-style: italic;">%         </span><span style="color: #93a1a1; font-style: italic;">process starts from 0 and process 0 is usually used as master process</span>&#57344;&#57345;
<span style="color: #93a1a1; font-style: italic;">% </span><span style="color: #93a1a1; font-style: italic;">value:  received variable</span>&#57344;&#57345;&#57345;
MPI_Finalize()&#57344;&#57345;&#57345;
</pre>
</div>

<p>
Process sequence control functions:
</p>
<ul class="org-ul">
<li>Function <code>MPI_Barrier(communicator)</code> will block the process until
all the processes reached this location.
</li>

<li>Function <code>MPI_Probe(RANK, TAG, COMM)</code> test if process [RANK] send
a message with TAG from communicator COMM. Block the process if
no message arrive.
</li>

<li>Function <code>MPI_Iprobe(RANK, TAG, COMM)</code> test if process [RANK] send
a message with TAG from communicator COMM. NOT block the process
if no message arrive.
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Summary</h3>
<div class="outline-text-3" id="text-2-3">
<p>
If you run the octave with MPI, you will find multiple octave
processes in the process monitor. Once you run your MPI program,
multiple clones of your program are craeted. Each of them is a
full clone, <i>which means the content before <code>MPI_Init</code> or
<code>MPI_Finalize</code> will be run in each process</i>. You can test it by
putting some random function before <code>MPI_Init</code> and see the
result. So, if you have a complex application and only want to use
MPI to accelerate a specific function, you should call the MPI
program as a independent application and get the result in
indirect way. For example, you can write the result into a file
and read it in your main application. But that would require a
distributed file system, which is out of the range of this article.
</p>
</div>
</div>
</div>




<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> C</h2>
<div class="outline-text-2" id="text-3">
<p>
The MPI interface for C language is much more powerful than that
for octave. But basically, it follows the same procedure:
initializing MPI environment (world), communication with process id
and message tag. I recommend the tutorial from
[<a href="https://computing.llnl.gov/tutorials/mpi/">https://computing.llnl.gov/tutorials/mpi/</a>] as a learning
material. It provides many exercises for basic features of MPI in
C.
</p>

<p>
Compile C source file using the command below, for more options,
please check the document of the version you installed.
</p>
<div class="org-src-container">

<pre class="src src-bash">mpicc [YOUR_C_SOURCE_FILE]
</pre>
</div>

<ul class="org-ul">
<li>Point-to-point communication
<ul class="org-ul">
<li>blocked
<code>MPI_Send(buffer,count,type,dest,tag,comm)</code>
<code>MPI_Recv(buffer,count,type,source,tag,comm, status)</code>
<ul class="org-ul">
<li><b>buffer</b>
Buffer is the variable you want to send or you want to
receive the data. In C, you have to pass the buffer by
reference <code>&amp;var</code>.
</li>
<li><b>count</b>
Count is the number of data element to be sent since with the
address/reference you send, the reciever can read an array.
</li>
<li><b>type</b>
MPI defiend series of data types for each C type. These types
are used for transfering between processes (on network for
clusters).
</li>
<li><b>dest/source</b>
The id (rank) of process you want to send or you want to
receive from.
</li>
<li><b>tag</b>
Tag is used to identify the data you received. Since MPI is
an interface that can be used to transfer data through
network, it is possible that the data you send later arrives
earlier. If the processes of your MPI application have to
communicate multiple times, tag is necessary to receive the
data and execute on them in correct order.
</li>
<li><b>comm</b>
The communicator or called MPI_COMM_WORLD, which is the
MPI context created at the beginning of application.
</li>
<li><b>status</b>
         Em, it is status, but I have no idea what it exactly is&#x2026;
</li>
</ul>
</li>
<li>non-blocked
<code>MPI_ISend(buffer,count,type,dest,tag,comm,request)</code>
<code>MPI_IRecv(buffer,count,type,source,tag,comm,request)</code>
First, I have to clarify the blocking/non-blocking concept in this
context. It is different to the block/non-block in multiple
thread programming. Blocking means the application will not
continue until the data in the application buffer is completely
sent out (through internet protocol). Non-blocking means the
application continue without waiting. Pay attention, it wait
until the data is <b>sent out</b>, not <b>received</b> by other
processes. And I think that is why tag is necessary in both
cases, because if the application block until data received,
the tag is not necessary and the application becomes
sequential. Besides, you cannot put receive before send because
it is blocked.

<p>
What is the benefit of non-blocking? You don&#8217;t have to wait for
the receiving of data. In the example code below, we can see
that processes receive data before sending them.
</p>
<div class="org-src-container">

<pre class="src src-C"><span style="color: #d33682;">#include</span> <span style="color: #2aa198;">"mpi.h"</span>&#57344;&#57345;&#57345;
<span style="color: #d33682;">#include</span> <span style="color: #2aa198;">&lt;stdio.h&gt;</span>&#57344;&#57345;&#57345;
&#57344;&#57345;&#57345;
<span style="color: #b58900;">main</span>(<span style="color: #268bd2;">int</span> <span style="color: #6c71c4;">argc</span>, <span style="color: #268bd2;">char</span> *<span style="color: #6c71c4;">argv</span>[])  {&#57344;&#57345;&#57345;
<span style="color: #268bd2;">int</span> <span style="color: #6c71c4;">numtasks</span>, <span style="color: #6c71c4;">rank</span>, <span style="color: #6c71c4;">next</span>, <span style="color: #6c71c4;">prev</span>, <span style="color: #6c71c4;">buf</span>[2], <span style="color: #6c71c4;">tag1</span>=1, <span style="color: #6c71c4;">tag2</span>=2;&#57344;&#57345;&#57345;
<span style="color: #268bd2;">MPI_Request</span> <span style="color: #6c71c4;">reqs</span>[4];&#57344;&#57345;&#57345;
<span style="color: #268bd2;">MPI_Status</span> <span style="color: #6c71c4;">stats</span>[4];&#57344;&#57345;&#57345;
&#57344;&#57345;&#57345;
MPI_Init(&amp;argc,&amp;argv);&#57344;&#57345;&#57345;
MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);&#57344;&#57345;&#57345;
MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);&#57344;&#57345;&#57345;
&#57344;&#57345;&#57345;
prev = rank-1;&#57344;&#57345;&#57345;
next = rank+1;&#57344;&#57345;&#57345;
<span style="color: #859900;">if</span> (rank == 0)  prev = numtasks - 1;&#57344;&#57345;&#57345;
<span style="color: #859900;">if</span> (rank == (numtasks - 1))  next = 0;&#57344;&#57345;&#57345;
&#57344;&#57345;&#57345;
MPI_Irecv(&amp;buf[0], 1, MPI_INT, prev, tag1, MPI_COMM_WORLD, &amp;reqs[0]);&#57344;&#57345;&#57345;
MPI_Irecv(&amp;buf[1], 1, MPI_INT, next, tag2, MPI_COMM_WORLD, &amp;reqs[1]);&#57344;&#57345;&#57345;
&#57344;&#57345;&#57345;
MPI_Isend(&amp;rank, 1, MPI_INT, prev, tag2, MPI_COMM_WORLD, &amp;reqs[2]);&#57344;&#57345;&#57345;
MPI_Isend(&amp;rank, 1, MPI_INT, next, tag1, MPI_COMM_WORLD, &amp;reqs[3]);&#57344;&#57345;&#57345;
&#57344;&#57345;&#57345;
      {  <span style="color: #859900;">do</span> some work  }&#57344;&#57345;&#57345;
&#57344;&#57345;&#57345;
MPI_Waitall(4, reqs, stats);&#57344;&#57345;&#57345;
&#57344;&#57345;&#57345;
MPI_Finalize();&#57344;&#57345;&#57345;
}&#57344;&#57345;&#57345;
</pre>
</div>
</li>
</ul>
</li>
<li>Collective communication
Basically, all the collective communications can be implemented
using the point-to-point communication methods.
<ul class="org-ul">
<li>MPI_Reduce
Split data to processes, gather the result as reducing
operation such as the sum of array.
</li>
<li>MPI_Scatter
       Split data from one processes to all the processes.
</li>
<li>MPI_Gather
       Gather data from all the processes to one process
</li>
<li>MPI_Barrier
       For synchronizing.
</li>
</ul>
</li>
<li>Derived data type
Though the MPI provides series of primitive types, it is also
useful to let useser to define their own data type. You can
define a variable of type and use it as the MPI type.
<ol class="org-ol">
<li>Contiguous
TODO
</li>
<li>Vector
TODO
</li>
<li>Indexed
TODO
</li>
<li>Struct
TODO
</li>
</ol>
</li>
<li>Group and communicator management
Group is an ordered set of processes, where the id(rank) of
process go from zero to N-1. Communicator encompoasses a group of
processes. Communicator that comprises all tasks is represented
by MPI_COMM_WORLD. They are the same thing for programmer.

<p>
Groups can be destroyed and created during execution. One process
can be in multiple groups.
</p>
</li>
<li>Virtual topology routine
Organize the processors in geometric way, like orginizing the
processors in a two-dimensional array.
</li>
</ul>
</div>
</div>




<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Performance Comparison</h2>
<div class="outline-text-2" id="text-4">
<p>
<iframe
src="https://docs.google.com/spreadsheets/d/1kOLf_azmw872VHm6lmOxsPUvpD0hs-ZvbPUJDOjMBFM/pubhtml?gid=0&amp;single=true&amp;widget=true&amp;headers=false"
width="900" height="600"></iframe>
</p>
</div>
</div>

]]></content>
</entry>

</feed>
