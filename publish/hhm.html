<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Introduction</a></li>
<li><a href="#sec-2">inference problems to solve:</a>
<ul>
<li><a href="#sec-2-1">compute the probability of a hhm generating an observation sequence.</a></li>
<li><a href="#sec-2-2">compute &#x2026;</a></li>
<li><a href="#sec-2-3">compute the most likely hidden states given an observation sequence</a></li>
</ul>
</li>
<li><a href="#sec-3">apply to tagging problem</a>
<ul>
<li><a href="#sec-3-1">parameters:</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Hidden Markov Machine (HHM) is used to process the sequential data. It
models the hidden states of observations. 
Given a sequence \(\mathbf{s}=(s_{1},\dots,s_{n})\) while \(s_{i}\) is
an single element and \(n\) is variable. 
HHM models the probability of generating the observation with hidden
units. 
Notice that one hidden unit corresponds to one observed element. 
Each hidden unit and each observed element have their own state, where
the state of hidden unit is called hidden state and the state of
observed element is called observed state. 
HHM model specify the probability of each hidden state generating each
observed state and the transition probability between hidden states. 
</p>

<p>
For example, if \(\mathbf{s}\) is a sentence, \(s_{i}\) can be a word, and
the length of sentence is variable. 
For POS tagging task, the hidden state can be assigned to different
POS labels, the number of hidden state is the number of POS labels. 
Tagging process is actually finding the hidden states of each word
that maximize the probability of HHM generating the sentence, which is
computed based on the probability of each POS label to word and the
probability of preceding POS label to succeeding POS label. 
</p>


<p>
For simplicity, we discuss the first order HHM here, in where the
state of the current hidden unit only rely on the one before it. 
A first order HHM contains two types of parameters:
</p>
<ul class="org-ul">
<li><b>Emission probabilities</b>: probability of hidden states generate the
</li>
</ul>
<p>
observed value \(\mathbf{B}\), where \(\mathbf{B}_{i,j}\) denotes the probability
of hidden unit \(i\) generating observed state \(j\).
</p>
<ul class="org-ul">
<li><b>Transition probabilities</b>: probability of hidden states given the
</li>
</ul>
<p>
precedent hidden states (can be more than one states) \(\mathbf{P}\)
where \(\mathbf{P}_{i,j}\) is the probability of hidden state \(j\) jumps
to hidden  state \(i\).
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">inference problems to solve:</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">compute the probability of a hhm generating an observation sequence.</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>solved by forward algorithm (\(\alpha\) algorithm)
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">compute &#x2026;</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>solved by backward algorithm (\(\beta\) algorithm)
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">compute the most likely hidden states given an observation sequence</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>input \(\mathbf{P}\), \(\mathbf{B}\), \(\mathbf{x}=(x_{1},\dots,x_{T})\)
</li>
<li>solved by Viterbi algorithm
<ol class="org-ol">
<li>the probability of state 1 is the emission probability multiple
prior probability of states \(\gamma_{1}=\mathbf{b}\).
</li>
<li>then we pass each observation_i (\(i \ge 2\)) to compute the
probability of current state. Expand \(\gamma_{i-1}\) to
\(\Gamma_{i-1}=[\gamma_{i-1};\dots;\gamma_{i-1}]\)
\(\gamma_{i}=\mathbf{b}\times
        \max(\mathbf{P}\times\Gamma_{i-1})\). 
</li>
<li>Save the preceding states that produce the current states. 
\(Ptr(i,i-1) = \{state_i\rightarrow
        argmax_{j}\mathbf{P}[i,j]\times\gamma_{i-1}[j]\}\)
</li>
<li>The optimal states is retrieved by saving back
pointers. 
</li>
</ol>
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">apply to tagging problem</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1">parameters:</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>we have \(\mathbf{P}\), which is the transition matrix between POS
</li>
<li>we have \(\mathbf{B}\) stores the probability of POS emit word
\(w\). 
denote \(ind(w)\) is the function that return the index of
word \(w\), \(\mathbf{B}_{i,j}\) is the probability of the POS \(i\)
emits word \(w\) where \(ind(w)=j\). 
</li>
</ul>
</div>
</div>
</div>
